\documentclass{beamer}
\usepackage{listings}
\lstset{
%language=C,
frame=single, 
breaklines=true,
columns=fullflexible
}
\usepackage{subcaption}
\usepackage{url}
\usepackage{tikz}
\usepackage{tkz-euclide} % loads  TikZ and tkz-base
%\usetkzobj{all}
\usetikzlibrary{calc,math}
\usepackage{float}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\renewcommand{\vec}[1]{\mathbf{#1}}
\usepackage[export]{adjustbox}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usetheme{Boadilla}

\title{Efficient Bernoulli Probability Distribution Estimation for Arithmetic Coding}
\author{Megh Shah}
\date{CS20BTECH11032}

\begin{document}
\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\frametitle{Abstract}
\begin{enumerate}
    \item This paper presents a novel method for Bernoulli probability distribution estimation, based on low pass filtering with varying dominant pole.
    \item This solution uses integer-only arithmetic, (and right and left shift operations) without multiplication or division, thus reducing the required resources of IoT devices.
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Maximum Likelihood Estimation (MLE)}
\begin{enumerate}
    \item Maximum likelihood estimation is used to estimate an unknown parameter $\theta$ for which the likelihood function $L(\theta|x)$ is largest, where $x$ is the recorded observation. 
    \item Hence we mean to find the value of the parameter for which the observation(s) recorded is (are) the most likely.
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{ML for Binomial Distribution}
For $x \sim \mbox{Bin}(n,p)$, known observation $x$, known $n$ and unknown $p$: 
\newline The likelihood function is: 
\begin{align}
    L(p|x) = \frac{n!}{x!(n-x)!}p^x(1-p)^{n-x}
\end{align}
We know $L(p|x)$ is a continuous, differentiable function.
\newline $L(p|x)=0$ for $p=0,1$ and for $0 \leq p \leq 1$, $L(p|x)>0$ 
\newline Hence atleast 1 maxima exists for $\hat{p}$ where $0 < \hat{p} < 1$
\newline Taking the natural logarithm on both sides and differentiating, we get:
\begin{align}
    \frac{1}{L(p|x)}\frac{d(L(p|x))}{dp} = 0 + x\frac{d(\ln p)}{dp} +(n-x)\frac{d(\ln(1-p))}{dp}
\end{align}
\end{frame}

\begin{frame}
\frametitle{ML for Binomial Distribution (Cont.)}
Setting $\frac{d(L(p|x))}{dp}=0$
\begin{align}
   0 &= x\frac{d(\ln p)}{dp} +(n-x)\frac{d(\ln(1-p))}{dp} \\
   0 &= \frac{x}{p} +\frac{x-n}{1-p} \\
  p(n-x) &= x(1-p) \\
  p &= \frac{x}{n}
\end{align}
Since the derivative is 0 at only one point, it is the maxima and $\hat{p} = x/n$
\end{frame}

\begin{frame}
\frametitle{Considering a Symbol Sequence for Data Compression}
Consider a sequence of symbols $y_i$ from the symbol set $S = \{S_0, S_1\}$, whose first k symbols are known. \newline Assuming that the symbol sequence is generated by a Bernoulli process with time-invariant probability distribution $p(y)$: 
\newline The probability of symbol $y$ occurring (using MLE): $\hat{p}_k(y) = x_y/k$ 
\newline Where $x_y$ is the number of occurrences of the symbol $y$ and $k$ is the number of symbols sampled. This can be rewritten as:
\begin{align}
    \hat{p}_k(y)=\bigg( \sum_{i=1}^keq(y,y_i) \bigg) / k
\end{align}
where the $\hat{p}_k(y)$ is the probability distribution estimated
from the first k symbols, and $eq$ is the symbol equality function defined as
\begin{align}
    eq(a,b) = \begin{cases} 1 &\mbox{if } a = b \\ 
0 & \mbox{if } a \neq b \end{cases}
\end{align}
\end{frame}

\begin{frame}
\frametitle{Considering a Symbol Sequence for Data Compression}
Now, consider an auxiliary binary sequence, $u_i$, formed from the symbol sequence as follows:
\begin{align}
    u_i =eq(y_i,S_0)
\end{align}
The binary sequence $u_i$ can be represented as a sum of two stochastic signals, which is the expected value of the sample $u_i$ (probability of the sample $y_i$ being equal to the $S_0$) and the zero-mean noise $n_i$
\begin{align}
    u_i =p_i(S_0)+n_i
\end{align}
The noise variance is a function of the probability $p_i(S_0)$ as in
\begin{align}
    E(n_i^2)-{(E(n_i))}^2 = p_i(S_0)(1-p_i(S_0))-0
\end{align}
Thus, the worst-case noise power occurs when the $p_i(S_0)$ is 0.5
\end{frame}

\begin{frame}
\frametitle{Considering a Symbol Sequence for Data Compression}
Since the symbol probability distribution $p_i(y)$ is slowly changing, the power spectral density of the signal $p_i(S_0)$ is concentrated in low frequency range ($p_i(S_0)$ is a narrow band signal). \newline So the signal to noise ratio can be improved by filtering $u_i$ with a low pass filter. \newline If the bandwidth is too narrow, then the filter output will be slow to converge to the true probability estimates. \newline And if the the bandwidth is too broad, then there will be a lot of noise. \newline Thus, the filter would have higher bandwidth in the beginning of the sequence $y_i$, to quickly identify initial conditions, but a reduced bandwidth later, to reject the noise.
\end{frame}

\begin{frame}
\frametitle{Low Pass Filter}
\begin{block}{Noise Power Spectral Density}
The noise power spectral density (PSD) specifies the average power of noise at different frequencies.
If we apply this noise to an LTI system, the transfer function of the system will determine the output average power at different frequencies. 
\end{block}
\begin{figure}
\includegraphics[scale=0.4]{P1.png}
\caption{Figure 1 shows the spectrum of a hypothetical noise source that exhibits the same average power at all frequencies, i.e. $S_X(f)=\eta$ where $\eta$ is a constant}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{Low Pass Filter (Cont.)}
In case if the system is an ideal low-pass filter with a DC gain of 1, after applying the filter, all of the noise frequency components in the stop-band will be suppressed and the frequency components in the pass-band will be unaffected.
\begin{figure}
\includegraphics[scale=0.4]{P2.png}
\caption{Figure 2 shows the application of an ideal low-pass filter on the previous hypothetical case}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{System Zeros, Poles and Transfer Functions}
Transfer functions, in general, of an electronic or control system component are a mathematical function which theoretically model the device's output for each possible input.
\newline The transfer function is a rational function in the complex variable $s = \sigma + j\omega$:
\begin{align}
    H(s)=\frac{b_ms^m+b_{m-1}s^{m-1}+...+b_1s+b_0}{a_ns^n+a_{n-1}s^{n-1}+...+a_1s+a_0}
\end{align}
Transfer functions are often written in terms of the factors of the numerator and denominator polynomials:
\begin{align}
    H(s)=\frac{N(s)}{D(s)}=\frac{b_m}{a_n}\frac{(s-z_1)(s-z_2)...(s-z_{m-1})(s-z_m)}{(s-p_1)(s-p_2)...(s-p_{n-1})(s-p_n)}
\end{align}
\end{frame}

\begin{frame}
\frametitle{System Zeros, Poles and Transfer Functions (Cont.)}
Now, the system zeros are simply defined to be roots of the numerator polynomial ($z_i$) and the system poles are defined to be the roots of the denominator polynomial ($p_i$).
\newline The poles and zeros are properties of the transfer function, and along with the gain constant $b_m/a_n$, they completely characterize the differential equation, and provide a total description of the system.
\begin{figure}
\includegraphics[scale=0.4]{P3.png}
\caption{The various locations of poles in the Complex plane and their corresponding homogeneous responses}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{Maximum Likelihood Filter}
For maximal filter length $N$ (at maximum, previous $N$ symbols are used for probability estimation):
\begin{align}
    h_k &= \begin{cases} \bigg( \sum_{i=1}^k T (eq(x_i,S_0)) \bigg) / k &\mbox{if } k < N  \\
    \bigg( \sum_{i=k-N+1}^k T (eq(x_i,S_0)) \bigg) / N &\mbox{if } k \geq N \end{cases} \\ 
\end{align}
And to remove the division by integers (a costly operation), we use the right shift operator:
\begin{align}
    w &= \lfloor \mbox{log}_2(\mbox{min}(k,N)) \rfloor \\
    h_k &= 2^{-w} \sum_{i=k-2^w+1}^k T (eq(x_i,S_0))
\end{align}
\end{frame}
\end{document}
